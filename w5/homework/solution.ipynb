{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1. Install Spark and PySpark\n",
    "\n",
    "* Install Spark\n",
    "* Run PySpark\n",
    "* Create a local spark session \n",
    "* Execute `spark.version`\n",
    "\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "        .master('local[*]')\n",
    "        .appName('test')\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. HVFHW February 2021\n",
    "\n",
    "Download the HVFHV data for february 2021:\n",
    "\n",
    "```bash\n",
    "https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet\n",
    "```\n",
    "\n",
    "Read it with Spark using the same schema as we did \n",
    "in the lessons. We will use this dataset for all\n",
    "the remaining questions.\n",
    "\n",
    "Repartition it to 24 partitions and save it to\n",
    "parquet.\n",
    "\n",
    "What's the size of the folder with results (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  288M  100  288M    0     0  12.0M      0  0:00:23  0:00:23 --:--:-- 12.9M\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir -p data/raw\n",
    "curl https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-02.parquet > data/raw/fhvhv_tripdata_2021-02.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/05 16:11:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=========>                                                (4 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/05 16:11:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================>                  (16 + 8) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/05 16:11:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/08/05 16:11:18 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('data/raw/fhvhv_tripdata_2021-02.parquet')\n",
    "df.repartition(24).write.parquet('data/part/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534M\tdata/part\n"
     ]
    }
   ],
   "source": [
    "!du -sh data/part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Count records \n",
    "\n",
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|432118|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select\n",
    "    count(1) as count\n",
    "from fhvhv\n",
    "where\n",
    "    pickup_datetime >= '2021-02-15' and pickup_datetime < '2021-02-16'\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|               date|longest_trip_time|\n",
      "+-------------------+-----------------+\n",
      "|2021-02-11 00:00:00|            75540|\n",
      "+-------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select\n",
    "    date_trunc('day', pickup_datetime) as date,\n",
    "    max(trip_time) as longest_trip_time\n",
    "from fhvhv\n",
    "group by 1\n",
    "order by longest_trip_time desc\n",
    "limit 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Most frequent `dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occurring `dispatching_base_num` \n",
    "in this dataset.\n",
    "\n",
    "How many stages this spark job has?\n",
    "\n",
    "> Note: the answer may depend on how you write the query,\n",
    "> so there are multiple correct answers. \n",
    "> Select the one you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|  freqs|\n",
      "+--------------------+-------+\n",
      "|              B02510|3233664|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select\n",
    "    dispatching_base_num,\n",
    "    count(1) as freqs\n",
    "from fhvhv\n",
    "group by 1\n",
    "order by freqs desc\n",
    "limit 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: 2 stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair. \n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- locationid: string (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read.csv('../../data/taxi+_zone_lookup.csv', header='true')\n",
    "df_zones.printSchema()\n",
    "df_zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(pair='East New York / East New York', freqs=45041)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select\n",
    "    concat(\n",
    "        ifnull(pzone.zone, 'Unknown'), ' / ', ifnull(dzone.zone, 'Unknown')\n",
    "    ) as pair,\n",
    "    count(1) as freqs\n",
    "from fhvhv\n",
    "left join zones pzone on pzone.locationid = fhvhv.PULocationID\n",
    "left join zones dzone on dzone.locationid = fhvhv.DOLocationID\n",
    "group by 1\n",
    "order by freqs desc\n",
    "limit 1\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question. Join type\n",
    "\n",
    "(not graded) \n",
    "\n",
    "For finding the answer to Q6, you'll need to perform a join.\n",
    "\n",
    "What type of join is it?\n",
    "\n",
    "And how many stages your spark job has?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: left join, 2 stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('data-engineering-zoomcamp-qXYECzdi-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4f79fffaa52f26b4b81f6511eff08be4302fbbe2a33d33c03dd2fe71115635e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
